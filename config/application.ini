[DEFAULT]
# if any default config all sections

[common]
home_path=./
raw_file_path=%(home_path)s/raw
preprocessed_file_path=%(home_path)s/preprocessed

#in seconds
query_interval=5
query_mean_interval=5

#used by preprocess.py
[queries]
Ax=SELECT * FROM Ax WHERE time > now() - 5m
Ay=SELECT * FROM Ay WHERE time > now() - 5m
Az=SELECT * FROM Az WHERE time > now() - 5m
Gx=SELECT * FROM Gx WHERE time > now() - 5m
Gy=SELECT * FROM Gy WHERE time > now() - 5m
Gz=SELECT * FROM Gz WHERE time > now() - 5m

[online_mean_config]
#time to considered for finding missing data, should be equal to max lag expected from senor_lag
#unit can be s, m, h. Corressponds to the time(second wise, minute wise, or hourly considered for missing data
#if sensor is sending data with a delay of 10 seconds, set the value to 10s
#if sensor lag is 5 hours, set the value to 5h
#minimum value cannot be less than 3, to make this work properly
#both sensor_lag and group by should have same unit(s,m,h)
sensor_lag=60s

#in terms of seconds, if missing data should be calculated for every second
#in terms of minutes, if missing data should be calculated for every minutes
#in terms of hours, if missing data should be calculated for every minutes
#both sensor_lag and group by should have same unit(s,m,h)
group_by=1s

#number of data points to be considered for making it as a missing data
#for example, this value is set to 10 and group_by it set to 1s, 
#assuming total number of data points send by sensor/second is less 10, 
#then this is considered a missing data and value is not set for that second

#If we talk about the original sensor which we place on the machine, 
#then we consider, 70 Hz and vibration data is in milliseconds, 
#so 70 samples for every millisecond, i.e, 70*60*60 for every minute
having=1

[forecast]
#query to predict forecasting
query=select * from Ax where time > now()- 24h
#query to check existance of data
query_exists=select value,time from Ax where time = $$input_time
[missing_interpolation]
#interpolation_start_time=120s
#time < now() - ${interpolation_start_time} and
interpolation_end_time=180s
data_required_to_skip_interpolation=20
count_query=select count(value) from Ax group by time 
query=select sum(value) as value from Ax where time > now() - ${interpolation_end_time}
    group by time(${online_mean_config:group_by})
interpolation_interval=299s    
#query to plot preprocessed data
[plot]
Ax=select * from Ax
Ay=select * from Ay

#used by preprocess_online_mean
[queries_mean]
Ax=select mean(value), count(value), time from Ax where time > now() - ${online_mean_config:sensor_lag} group by time(${online_mean_config:group_by}) offset 1  
Ay=select mean(value), count(value), time from Ay where time > now() - ${online_mean_config:sensor_lag} group by time(${online_mean_config:group_by}) offset 1  
Az=select mean(value), count(value), time from Az where time > now() - ${online_mean_config:sensor_lag} group by time(${online_mean_config:group_by}) offset 1  
Gx=select mean(value), count(value), time from Gx where time > now() - ${online_mean_config:sensor_lag} group by time(${online_mean_config:group_by}) offset 1  
Gy=select mean(value), count(value), time from Gy where time > now() - ${online_mean_config:sensor_lag} group by time(${online_mean_config:group_by}) offset 1  
Gz=select mean(value), count(value), time from Gz where time > now() - ${online_mean_config:sensor_lag} group by time(${online_mean_config:group_by}) offset 1  

[influxdb]
raw_data_host=localhost
raw_data_port=8086
raw_database=sensors

preprocessed_data_host=localhost
preprocessed_data_port=8086
preprocessed_database=preprocessed

#not used
interpolated_db_host=localhost
interpolated_db_port=8086
interpolated_db=interpolated

forecast_db_host=localhost
forecast_db_port=8086
forecast_db=forecast

client_path=E:\influxdb-1.7.10-1